{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_PA03GbeyIG"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNP3CvLjeyS5"
      },
      "source": [
        "## Training an MLP model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHFmRS8zeyVa"
      },
      "source": [
        "Install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q30A61o_e4Ub"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install flax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji1Y9b6efPGe"
      },
      "source": [
        "Define the imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtWrCIJde4W0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as skdata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax\n",
        "from flax import linen as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9g6SiQFK3nl"
      },
      "source": [
        "Load and preprocess the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzOMfk5CK2rb"
      },
      "outputs": [],
      "source": [
        "iris = skdata.load_iris()\n",
        "X = iris.data  # shape: (150, 4)\n",
        "y = iris.target  # Labels: 0, 1, 2\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1337,\n",
        ")\n",
        "\n",
        "# Convert to JAX arrays\n",
        "X_train = jnp.array(X_train)\n",
        "y_train = jnp.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "DRw-63al0s9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4tF1vMQKjJJ"
      },
      "source": [
        "Define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZn4fyESJiW-"
      },
      "outputs": [],
      "source": [
        "class MLPClassifierSmall(nn.Module):\n",
        "    num_classes: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "        x = nn.Dense(8)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(16)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(8)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.num_classes)(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now instantiating the training loop \"state\":\n",
        "- params: model weights\n",
        "- opt_state: optimizer internal state\n",
        "- rng: randomness (shuffling, dropout, etc.)\n",
        "- step/epoch counters\n"
      ],
      "metadata": {
        "id": "4Kyq8XeJEduf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWDMrbWLbz9"
      },
      "source": [
        "Finally, run a script!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzeu73yxHkq3"
      },
      "outputs": [],
      "source": [
        "# HPs\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "num_classes = 3\n",
        "input_features = X.shape[1]\n",
        "\n",
        "# Initialize the model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "model = MLPClassifierSmall(num_classes=num_classes)\n",
        "params = model.init(rng, jnp.ones((1, input_features)))\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    one_hot = jax.nn.one_hot(y, num_classes)\n",
        "    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    predicted_classes = jnp.argmax(logits, axis=1)\n",
        "    correct_predictions = predicted_classes == y\n",
        "    return jnp.mean(correct_predictions)\n",
        "\n",
        "\n",
        "# A single update step\n",
        "@jax.jit\n",
        "def update(params, opt_state, x, y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "num_train = X_train.shape[0]\n",
        "num_test = X_test.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "print(f\"Accuracy before training: {accuracy(params, X_test, y_test)}\")\n",
        "\n",
        "# Training loop!\n",
        "for epoch in range(num_epochs):\n",
        "    # Shuffle training data\n",
        "    rng, perm_key = jax.random.split(rng)\n",
        "    permutation = jax.random.permutation(perm_key, num_train)\n",
        "    X_train_shuffled = X_train[permutation]\n",
        "    y_train_shuffled = y_train[permutation]\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # Process training batches\n",
        "    for i in range(0, num_train, batch_size):\n",
        "        batch_x = X_train_shuffled[i:i+batch_size]\n",
        "        batch_y = y_train_shuffled[i:i+batch_size]\n",
        "        params, opt_state, loss = update(params, opt_state, batch_x, batch_y)\n",
        "        epoch_train_loss += loss * batch_x.shape[0]\n",
        "\n",
        "    epoch_train_loss /= num_train\n",
        "    train_losses.append(float(epoch_train_loss))\n",
        "\n",
        "print(f\"Accuracy after training: {accuracy(params, X_test, y_test)}\")\n",
        "\n",
        "# Plot training vs testing loss.\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
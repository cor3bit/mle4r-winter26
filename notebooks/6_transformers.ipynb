{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Attention and Transformer"
      ],
      "metadata": {
        "id": "D71_dJG1enrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: ensure Runtime GPU is selected!"
      ],
      "metadata": {
        "id": "6yWK6s24WmOw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHFmRS8zeyVa"
      },
      "source": [
        "Install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q30A61o_e4Ub"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install flax wandb tensorboardX tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji1Y9b6efPGe"
      },
      "source": [
        "Define the imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtWrCIJde4W0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as skdata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax\n",
        "from flax import linen as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminaries"
      ],
      "metadata": {
        "id": "DLQRaWoNrClD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**\n",
        "\n",
        "Embeddings are a way to convert discrete tokens (like words or indices) into continuous vectors. Instead of representing a word as a huge, sparse one-hot vector (mostly zeros, one one), an embedding maps it into a smaller, dense vector space where similar tokens can have similar representations.\n",
        "\n",
        "**Why use embeddings?**\n",
        "\n",
        "- **Efficiency:** One-hot vectors are high-dimensional and sparse (e.g., a vocabulary of 100 words gives a 100-dimensional vector, with 99 zeros).  \n",
        "- **Expressiveness:** Dense vectors (say, 256 dimensions) can capture more nuanced relationships between words (similar words can end up with similar vectors).  \n",
        "- **Learning:** Embeddings are learnable parameters. Instead of manually designing features, the model learns the best way to represent tokens for the task at hand.\n",
        "\n",
        "**How does it work?**\n",
        "\n",
        "Imagine you have a vocabulary of 100 tokens. A one-hot vector for any token is a vector of length 100 with a single 1 at the token’s index and 0 everywhere else. An embedding layer is essentially a lookup table (a matrix) of shape `(vocab_size, embedding_dim)`. When you \"look up\" a token, you use its one-hot vector to select the corresponding row from this matrix.\n",
        "\n",
        "$$\n",
        "\\text{embedding} = \\mathbf{onehot} \\times E\n",
        "$$\n",
        "\n",
        "Because $ \\mathbf{onehot} $ has all zeros except a one at the token index, this effectively selects the row of $E $ corresponding to that token."
      ],
      "metadata": {
        "id": "AB-tk3d8xCOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10\n",
        "embed_dim = 3\n",
        "\n",
        "# Create a random embedding matrix of shape (10, 3)\n",
        "# Will be learned during training!\n",
        "embedding_matrix = np.random.randn(vocab_size, embed_dim)\n",
        "print(\"Embedding Matrix:\\n\", embedding_matrix)\n",
        "\n",
        "# Let's say our token index is 2\n",
        "token_index = 2\n",
        "\n",
        "# Create one-hot vector for token_index 2\n",
        "one_hot = np.zeros(vocab_size)\n",
        "one_hot[token_index] = 1\n",
        "print(\"One-hot vector:\\n\", one_hot)\n",
        "\n",
        "# Get embedding by dot product: (1,5) * (5,3) = (1,3)\n",
        "embedding = one_hot.dot(embedding_matrix)\n",
        "print(\"Resulting embedding:\\n\", embedding)\n",
        "\n",
        "# Alternatively, simply index the embedding matrix:\n",
        "embedding_lookup = embedding_matrix[token_index]\n",
        "print(\"Embedding via lookup:\\n\", embedding_lookup)"
      ],
      "metadata": {
        "id": "uNZxKaOcy37o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head (Dot-Product) Attention**\n",
        "\n",
        "See the original paper https://arxiv.org/abs/1706.03762\n",
        "\n",
        "Why attention? Intuitively, processing the sequence word for word is not always optimal:\n",
        "\n",
        "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png\" width=\"500\">\n",
        "\n",
        "Access all sequence elements at each time step!\n",
        "\n",
        "At its core, attention works by:\n",
        "\n",
        "1. **Creating Queries, Keys, and Values:**  \n",
        "   For each token, we create three vectors:\n",
        "   - **Query (Q):** Represents what information the token is looking for in the rest of the sequence.\n",
        "   - **Key (K):** Represents the token’s characteristics that might be useful for other tokens.\n",
        "   - **Value (V):** Contains the actual information of the token.\n",
        "   \n",
        "   In many cases, these vectors are created by applying different learned transformations to the original embedding.\n",
        "\n",
        "2. **Calculating Attention Scores:**  \n",
        "   For a given token, we want to figure out how much attention it should pay to every other token. We do this by comparing its query with the keys of all tokens:\n",
        "   - **Dot Product:** We calculate a dot product between the query of $i$-th token and the key of each token. This measures their similarity.\n",
        "   - **Scaling:** The dot product is scaled (divided by the square root of the dimension of the vectors) to prevent the numbers from getting too large, which helps stabilize learning.\n",
        "\n",
        "3. **Applying Softmax:**  \n",
        "   The raw attention scores are then passed through a softmax function. This converts the scores into probabilities (or weights) that add up to 1. The softmax emphasizes the tokens with higher similarity scores, so $i$-th token will \"attend\" more to those tokens.\n",
        "\n",
        "4. **Aggregating Values:**  \n",
        "   Finally, we use these attention weights to create a new representation for token $i$:\n",
        "   - Multiply each token's value vector by its corresponding attention weight.\n",
        "   - Sum up these weighted vectors.  \n",
        "     \n",
        "   This weighted sum is the attention output for token $i$—it’s a blend of information from all tokens, focused according to the attention weights.\n",
        "\n",
        "\n",
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png\" width=\"800\">\n",
        "\n",
        "\n",
        "\n",
        "Images: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention"
      ],
      "metadata": {
        "id": "HLxpTMHIkhPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----- Step 1: Define a simple embedding matrix for 5 tokens -----\n",
        "embedding_matrix = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],   # Token 0\n",
        "    [0.5, 0.6, 0.7, 0.8],   # Token 1\n",
        "    [0.9, 1.0, 1.1, 1.2],   # Token 2\n",
        "    [1.3, 1.4, 1.5, 1.6],   # Token 3\n",
        "    [1.7, 1.8, 1.9, 2.0]    # Token 4\n",
        "])\n",
        "\n",
        "# Assume our sequence consists of tokens: [1, 2, 3]\n",
        "sequence_ids = [1, 2, 3]\n",
        "\n",
        "embeddings = embedding_matrix[sequence_ids]  # Shape: (num_tokens, embed_dim)\n",
        "\n",
        "# For simplicity, we use the embeddings directly as Q, K, and V\n",
        "Q = embeddings.copy()  # Queries for all tokens\n",
        "K = embeddings.copy()  # Keys for all tokens\n",
        "V = embeddings.copy()  # Values for all tokens\n",
        "\n",
        "# Let's focus on computing the attention for token 2 (which is at index 1 in our sequence)\n",
        "target_index = 1  # This corresponds to token 2\n",
        "\n",
        "# ----- Step 2: Compute the scaled dot product scores -----\n",
        "embed_dim = embeddings.shape[1]\n",
        "num_tokens = embeddings.shape[0]\n",
        "scores = np.zeros(num_tokens)\n",
        "\n",
        "# Compute dot product between Q[target_index] and every key K[j]\n",
        "for j in range(num_tokens):\n",
        "    dot_product = 0.0\n",
        "    for i in range(embed_dim):\n",
        "        dot_product += Q[target_index, i] * K[j, i]\n",
        "\n",
        "    # Scale the score by the square root of the embedding dimension for stability\n",
        "    scores[j] = dot_product / np.sqrt(embed_dim)\n",
        "\n",
        "print(\"Raw attention scores for token 2:\", scores)\n",
        "\n",
        "# ----- Step 3: Apply Softmax to get attention weights -----\n",
        "def softmax(x):\n",
        "    # Subtracting the max for numerical stability\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "attn_weights = softmax(scores)\n",
        "print(\"Attention weights for token 2:\", attn_weights)\n",
        "\n",
        "# ----- Step 4: Compute the attention output for token 2 -----\n",
        "# This is the weighted sum of the value vectors from all tokens\n",
        "attention_output = np.zeros(embed_dim)\n",
        "for j in range(num_tokens):\n",
        "    for i in range(embed_dim):\n",
        "        attention_output[i] += attn_weights[j] * V[j, i]\n",
        "\n",
        "print(\"Attention output for token 2:\", attention_output)"
      ],
      "metadata": {
        "id": "ZoA5Db7C2_kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Transformer in Flax"
      ],
      "metadata": {
        "id": "AMd-Guea016M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask\n",
        "jnp.tril(jnp.ones((10, 10)))"
      ],
      "metadata": {
        "id": "wHBzmIdkqGr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb61939e-e9ae-416f-8b75-9ed808be0782_1456x1392.png\" width=\"600\">\n",
        "\n",
        "Image: https://magazine.sebastianraschka.com/p/building-a-gpt-style-llm-classifier\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cz4HvflL56hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoLM(nn.Module):\n",
        "    vocab_size: int\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    head_size: int = 32\n",
        "    dropout_rate: float = 0.2\n",
        "    embed_size: int = 256\n",
        "    block_size: int = 64\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = True):\n",
        "        # x: (8, 10) -- batch of 8 sequences, each of length 10 (token indices)\n",
        "\n",
        "        seq_len = x.shape[1] # seq_len = 10\n",
        "\n",
        "        # from index in the vocab to a dense vector\n",
        "        # Output: (8, 10, 256)\n",
        "        x = nn.Embed(self.vocab_size, self.embed_size)(x)\n",
        "\n",
        "        # positional embedding\n",
        "        # Output: (10, 256), then broadcast to (8, 10, 256) for addition\n",
        "        x = x + nn.Embed(self.block_size, self.embed_size)(jnp.arange(seq_len))\n",
        "\n",
        "        # for N layers\n",
        "        for _ in range(self.num_layers):\n",
        "            # Pre-layer normalization:\n",
        "            # nn.LayerNorm()(x) normalizes each token's 256-d vector\n",
        "            # Shape remains (8, 10, 256)\n",
        "            x_norm = nn.LayerNorm()(x)\n",
        "\n",
        "            # Self-Attention Block with Residual Connection\n",
        "            # - Input: x_norm (8, 10, 256)\n",
        "            # - Internally, each head projects to a 32-d space\n",
        "            #   Thus, for 8 heads: total dimension = 8 * 32 = 256\n",
        "            # - The attention mechanism outputs a tensor of shape (8, 10, 256)\n",
        "            # - The causal mask ensures each position only attends to previous ones\n",
        "            # !!! output matches the initial x, the process repeats num_layers times\n",
        "            attn_out = nn.MultiHeadDotProductAttention(\n",
        "                num_heads=self.num_heads,\n",
        "                qkv_features=self.head_size,\n",
        "                out_features=self.head_size * self.num_heads,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "            )(\n",
        "                x_norm,  # queries: (8, 10, 256)\n",
        "                x_norm,  # keys:    (8, 10, 256)\n",
        "                mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),  # (10, 10)\n",
        "                deterministic=not training,\n",
        "            )\n",
        "\n",
        "            # Residual connection: add attention output to original x\n",
        "            # Shape remains (8, 10, 256)\n",
        "            x = x + attn_out\n",
        "\n",
        "            # Feedforward (MLP) Block with Residual Connection\n",
        "            # extra \"x +\" helps with gradient flow and retains information from earlier layers\n",
        "            x = x + nn.Sequential([\n",
        "                nn.Dense(4 * self.embed_size), # Expand: (8, 10, 256) -> (8, 10, 1024)\n",
        "                nn.relu, # Activation: (8, 10, 1024)\n",
        "                nn.Dropout(self.dropout_rate, deterministic=not training), # (8, 10, 1024)\n",
        "                nn.Dense(self.embed_size), # Project: (8, 10, 1024) -> (8, 10, 256)\n",
        "            ])(nn.LayerNorm()(x))\n",
        "\n",
        "        # Final Layer Normalization:\n",
        "        # Normalizes final representation at each token\n",
        "        # Shape: (8, 10, 256)\n",
        "        x = nn.LayerNorm()(x)\n",
        "\n",
        "        # Output Projection:\n",
        "        # Projects each 256-d token representation to logits over the vocabulary\n",
        "        # Dense layer: (8, 10, 256) -> (8, 10, vocab_size) = (8, 10, 100)\n",
        "        return nn.Dense(self.vocab_size)(x)"
      ],
      "metadata": {
        "id": "NdHXRJrteoO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model and run inference"
      ],
      "metadata": {
        "id": "r8cQ4W8nmmJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "key = jax.random.PRNGKey(1337)\n",
        "mini_transformer = NanoLM(vocab_size=100)\n",
        "\n",
        "# Example input: batch of token sequences (batch_size=8, seq_len=10)\n",
        "x = jnp.ones((8, 10), dtype=jnp.int32)\n",
        "\n",
        "# Initialize parameters\n",
        "params = mini_transformer.init(key, x)\n",
        "\n",
        "# Forward pass\n",
        "y = mini_transformer.apply(params, x, False)\n",
        "\n",
        "# Predict the next token at every (!) position\n",
        "# aka \"teacher forcing\" - helps the model learn the structure of the language by maximizing the likelihood of the entire sequence\n",
        "# if we only produced an output for the end token,\n",
        "# we'd lose valuable learning signals from every intermediate step\n",
        "y.shape"
      ],
      "metadata": {
        "id": "7YgGcotVe8Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(params):\n",
        "    leaves = jax.tree_util.tree_leaves(params)\n",
        "    return sum(x.size for x in leaves)\n",
        "\n",
        "n_params = count_params(params)\n",
        "print(f\"Total number of parameters: {n_params:,}\")"
      ],
      "metadata": {
        "id": "TfVuMakSwBnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def params_size_in_bytes(params):\n",
        "    leaves = jax.tree_util.tree_leaves(params)\n",
        "    total_bytes = sum([x.size * x.dtype.itemsize for x in leaves])\n",
        "    return total_bytes\n",
        "\n",
        "size_bytes = params_size_in_bytes(params)\n",
        "print(\"Total parameters size: {:.2f} MB\".format(size_bytes / (1024 ** 2)))"
      ],
      "metadata": {
        "id": "sKgG9sUYvy-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Makes sense, since with 3,426,468 parameters and assuming 32-bit (4-byte) floats, the total parameter memory is roughly:\n",
        "\n",
        "$$\n",
        "3,426,468 \\times 4 \\text{ bytes} \\approx 13,705,872 \\text{ bytes} \\approx 13.07 \\text{ MB}\n",
        "$$"
      ],
      "metadata": {
        "id": "BQGQjg50w02X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resume**:\n",
        "\n",
        "<img src=\"https://pbs.twimg.com/media/GCnZNRraAAE9HAx?format=png&name=small\" width=\"600\">\n",
        "\n",
        "Source: https://x.com/srush_nlp/status/1741161984928920027"
      ],
      "metadata": {
        "id": "9XUHlv7hhMog"
      }
    }
  ]
}